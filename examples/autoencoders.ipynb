{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders for dimensionality reduction and representation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import common libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and change directory \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/ColabNotebooks/SACAC-EDA-2025/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# pandas display format: two decimals\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also reuse the `plotAndColorLatentVariables` function developed during the dimensionality reduction session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that accepts scores (more generally, latent variables) \"L\" and dataframe \"df\" as inputs, with optional input \"alpha\"\n",
    "def plotAndColorLatentVariables(L, df, alpha = 0.5): \n",
    "    I = np.ceil(np.sqrt(df.shape[1])).astype(int)   # Determine the number of rows in the subplot grid\n",
    "    J = np.ceil(df.shape[1]/I).astype(int)          # Determine the number of columns in the subplot grid\n",
    "    fig, ax = plt.subplots(I,I, figsize = (20,20))  # Create the subplot grid\n",
    "    \n",
    "    for i in range(I):                       # Loop over the rows\n",
    "        for j in range(I):                   # Loop over the columns\n",
    "            k = I*i+j-1                      # Compute the index of the subplot\n",
    "            if k == -1:     # Colour the plot according to the dataframe index\n",
    "                ax[i,j].scatter(L[:, 0], L[:, 1], alpha = alpha, c=df.index, cmap='viridis')\n",
    "                ax[i,j].set_title('Index')\n",
    "                \n",
    "            elif k <= (df.shape[1]-1):  # Colour the plot according to the dataframe columns\n",
    "                ax[i,j].scatter(L[:, 0], L[:, 1], alpha = alpha, c=df.iloc[:,k], cmap='viridis')\n",
    "                ax[i,j].set_title(df.columns[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous session, we used the very artificial moons dataset for illustrative purposes, and the ver reall iron ore flotation datasets for practical application. In this session, we will introduce a synthetic, but believable dataset: the mixing tank dataset.\n",
    "\n",
    "The mixing tank is a commonly used example in process control. Two streams feed the mixing tank. The first has a variable flowrate $ F_0(t) $ and solute concentration $ C_0(t)$, which act as the disturbances to the system. The second is pure solvent ($C_1 = 0$), and the flowrate is manipulated to control the outlet solute concentration. The flowrate out of the tank $ F_2 $ is manipulated to control the liquid level in the tank. \n",
    "\n",
    "The system is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the mixing tank diagram\n",
    "from IPython.display import display, Image\n",
    "display(Image(filename='mixingTank.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data from `mixingTank_dataset.csv`, and immediately scale it so that the features have unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "df_mix = pd.read_csv('../data/mixingTank_dataset.csv')\n",
    "X = StandardScaler().fit_transform(df_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by performing PCA and considering the cumulative variance explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "T = pca.fit_transform(X)\n",
    "\n",
    "index=range(1,pca.n_components_+1)  # Used for the x-axis\n",
    "plt.bar(index, pca.explained_variance_ratio_, label='Individual components')\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cumulativeFractionVarianceExplained = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(index, cumulativeFractionVarianceExplained, 'o-', label='Cumulative variance explained')\n",
    "\n",
    "# Additional plot formatting\n",
    "plt.hlines(0.9, 0, pca.n_components_, color='k', ls='--', label='90 percent of total variance')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained variance fraction')\n",
    "plt.grid(True)  # Add grid\n",
    "plt.legend()\n",
    "\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first two PCs retain almost 80% of the variance, and the first four PCs retain >95% thereof. Let's visualise the first two PCs, coloured according to the measured variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAndColorLatentVariables(T, df_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do see two clusters, which we can attempt to identify using k-means or DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(T[:,:4])\n",
    "\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=10)\n",
    "dbscan.fit(T[:,:4])\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize=(15,5))\n",
    "ax[0].scatter(T[:, 0], T[:, 1], c=kmeans.labels_, cmap='viridis')\n",
    "ax[0].set_title('K-means clustering')\n",
    "ax[1].scatter(T[:, 0], T[:, 1], c='black', alpha=0.5, marker='x', s = 5)\n",
    "ax[1].scatter(T[dbscan.labels_ != -1, 0], T[dbscan.labels_ != -1, 1], c=dbscan.labels_[dbscan.labels_ != -1], cmap='viridis')\n",
    "ax[0].set_title('DBSCAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither clustering algorithm appears to give particularly satisfying results. You may continue to visualise the clusters using the time series data for practice. However, in the section below, we will apply an alternative dimensionality reduction approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks as autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a neural network with the aim of reconstructing the inputs after passing the inputs through an encoding (i.e., bottleneck) layer consisting of only two features; this type of neural network is often referred to as an autoencoder. The two-dimensional latent variable space must capture the characteristics of the six dimensional input features as effectively as possible. We refer to this as representation learning, that is, learning a lower dimensional representation of the inputs. \n",
    "\n",
    "We will use `keras` to construct an autoencoder with three dense hidden layers, a high-level API to the incredibly powerful TensorFlow package. \n",
    "\n",
    "* The `keras.Sequential` method is used to stack different layers of a neural network, making for very easy deep learning.\n",
    "* The `keras.Input` method simply accepts a fixed number of inputs and prepares the connections to subsequent layers\n",
    "* The `keras.Dense` method creates a dense (fully connected) neural network layer\n",
    "\n",
    "We use three activation functions:\n",
    "* The ReLU (rectified linear) activation function is very commonly used in deep learning, enabling non-linearity but negating many problematice effects associated with sigmoid or $\\tanh$ activation functions.\n",
    "* The $\\tanh$ activation limits outputs to lie between -1 and 1, which is useful to constrain the encoding layer\n",
    "* When no activation function is specified, a linear activation function is used, which simply gives the weighted sum of the outputs from the previous layer.\n",
    "\n",
    "It is common to construct an autoencoder in a symmetric fashion (notice the) number of neurons in the dense layers. There are many different neural network architectures and different types of units (e.g., convolutional filters, recurrent neurons, etc.) and a suitable design should be found using system insight and suitable training / testing / validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "nLayers_1 = 16  # Number of neurons in the first layer\n",
    "nLayers_2 = 8  # Number of neurons in the second layer\n",
    "\n",
    "# We define the encoder and decoder separately. The encoder can later be used to calculate the latent variables.\n",
    "aeEncoder = keras.Sequential([\n",
    "    keras.layers.Input(shape=(T[:,:4].shape[1],)),    # Input layer\n",
    "    keras.layers.Dense(nLayers_1, activation='relu'),  # First hidden layer with ReLU activation function\n",
    "    keras.layers.Dense(nLayers_2, activation='relu'),  # Second hidden layer with ReLU activation function\n",
    "    keras.layers.Dense(2, activation='tanh'),   # Encoding layer, with tanh activation function to limit the latent variables between [-1,1]\n",
    "])\n",
    "\n",
    "aeDecoder = keras.Sequential([\n",
    "    keras.layers.Dense(nLayers_2, activation='relu'),   # Second decoding layer with ReLU activation function\n",
    "    keras.layers.Dense(nLayers_1, activation='relu'),   # Third decoding layer with ReLU activation function\n",
    "    keras.layers.Dense(T[:,:4].shape[1])  # Output layer, with linear activation function to achieve any possible output value\n",
    "])\n",
    "\n",
    "# Autoencoder (combined model)\n",
    "autoencoder = keras.Sequential([\n",
    "    aeEncoder,\n",
    "    aeDecoder\n",
    "])\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "history=autoencoder.fit(T[:,:4], T[:,:4], epochs=50, batch_size=16)\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = aeEncoder.predict(T[:,:4])\n",
    "\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(L)\n",
    "\n",
    "dbscan = DBSCAN(eps=0.02, min_samples=5)\n",
    "dbscan.fit(L)\n",
    "\n",
    "df_mix_clustered = df_mix.copy()\n",
    "df_mix_clustered['KMeans'] = kmeans.labels_\n",
    "df_mix_clustered['DBSCAN'] = dbscan.labels_\n",
    "df_mix_clustered.loc[df_mix_clustered['DBSCAN'] == -1, 'DBSCAN'] = np.nan\n",
    "\n",
    "plotAndColorLatentVariables(L, df_mix_clustered, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of features to visualize\n",
    "selected = ['F0', 'F1', 'C0', 'F2', 'h', 'C2']\n",
    "\n",
    "fig,ax = plt.subplots(len(selected), 1, figsize=(12,20), sharex= True)\n",
    "for (i, col) in enumerate(selected):\n",
    "    ax[i].plot(df_mix.index, df_mix[col], 'k', alpha = 0.1)       # Line plot of features\n",
    "    ax[i].scatter(df_mix.index, df_mix[col], s = 5, c=df_mix_clustered['KMeans']) # Scatter plot of features colored by cluster\n",
    "    ax[i].set_ylabel(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the `keras` API simplifies the implementation of neural networks, many challenges remain: neural networks are difficult to design and are prone to overfitting, particularly on smaller datasets. Modern architectures (e.g., LSTMs) hold promise for time series analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
